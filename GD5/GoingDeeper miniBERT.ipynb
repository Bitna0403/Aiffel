{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5c20ee",
   "metadata": {},
   "source": [
    "# Project: Creating Mini BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2543e751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Checking tf version and gpu\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f2b79",
   "metadata": {},
   "source": [
    "## 1. Preparing the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8df56a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "prefix = 'ko_8000_jp'\n",
    "vocab_size = 8000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # Maximum length of the sentence\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # User-defined token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6764ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"./{prefix}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "729d663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list of words excluding seven special tokens\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a87f18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8979a95",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing (1) Creating MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35a1b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    Create the mask\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: Number of masked tokens (15% of total tokens)\n",
    "    :param vocab_list: vocab list (random token)\n",
    "    :return tokens: masked tokens\n",
    "    :return mask_idx: masked token's index\n",
    "    :return mask_label: masked token's original value\n",
    "    \"\"\"\n",
    "    # To mask at the word level, split the indices by spaces\n",
    "    cand_idx = []  # Index array at the word level\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\" signifies the beginning of a word\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # To create a random mask, shuffle the order (shuffle)\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # The masked value\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # Stop if the current count of masked tokens exceeds 15%\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # Skip if the total masked token count, including the upcoming ones, exceeds 15%\n",
    "            continue\n",
    "        dice = random.random()  # Probability value between 0 and 1\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "    # Extract mask_idx and mask_label after sorting mask_lms (using sorted)\n",
    "    mask_lms = sorted(mask_lms, key=lambda m: m[\"index\"])\n",
    "    mask_idx = [m[\"index\"] for m in mask_lms]\n",
    "    mask_label = [m[\"label\"] for m in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9596f",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing (2) Creating Next Sentence Prediction (NSP) Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0ff8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    Reducing the lengths of tokens_a and tokens_b with a maximum length of max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: The maximum length of two tokens\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3586424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    Generate pretraining data per document\n",
    "    \"\"\"\n",
    "    # for [CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # Add data for pretraining on a line-by-line \n",
    "        current_length += len(doc[i])  # Number of tokens in the current_chunk\n",
    "        # If it is the last line or its length is greater than or equal to max_seq\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  \n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # Swap with a 50% probability\n",
    "                is_next = 0    # False\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1   # True\n",
    "            # Adjust the length if it exceeds max_seq\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "           #######################################\n",
    "\n",
    "            # Create tokens and segments\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c2253",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing (3) Completing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8a92cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the file\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# Check the line count\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7297b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" Creating pretraining data \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # Create a vocab_list excluding seven special characters\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # Exclude the word list that includes 'unknown' during creation \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # Check the line count\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # If the line is empty\n",
    "                    save_pretrain_instances(out_f, doc)\n",
    "                    doc = []\n",
    "                else:  # Tokenize and save to doc if the line is not empty\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if len(pieces) == 0:\n",
    "                        continue\n",
    "                    doc.append(pieces)\n",
    "            if 1 < len(doc):  # If there are unprocessed documents at the end\n",
    "                save_pretrain_instances(out_f, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0634d21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebdbd8659974a29a9fc5e5bd56e1c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc1720e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918189"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of line\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c71e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    :Load data required for training\n",
    "    :param vocab: vocabulary\n",
    "    :param filename: preprocessed json file\n",
    "    :param n_seq: sequence length (number of sequences)\n",
    "    :param count: data limit (None for all)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # Limit the number of data\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # When 'np.memmap' is used, it enables processing large-scale data even with limited memory\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f477ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b8c367b91741dcb46645fbaecc939a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2481037652.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_31/2481037652.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_31/2481037652.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a45d3b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5,   10, 1605, 3599, 1755, 3630,   41, 3644,  830, 3624, 1135,\n",
       "           52, 3599,   13,   81,   87, 1501, 2247,   25, 3779, 3873, 3667,\n",
       "         3631, 3813, 3873, 4196, 3636, 3779, 3601,  249, 3725, 1232,   33,\n",
       "           52, 3599,    6,    6,    6, 6322, 2780,   14, 1509,  168, 3877,\n",
       "          414,  165, 1697, 4290, 3873, 3703, 3683,  593,   21, 5007,  399,\n",
       "         1927, 3607,    6,    6,    6,    6,    6,    6,  103, 4313, 4290,\n",
       "          613, 3638, 3718,   98, 3878, 3656,  256, 2543,  309,  337, 3735,\n",
       "          181, 3616, 3603,  489,  376, 3599,    4,    6,    6,  207, 3714,\n",
       "            6, 1042,  103, 3610, 3686, 3718,    6,    6,   37, 3418,  416,\n",
       "          810, 3666, 3625,  131, 3662,    7, 3629,  203,  241, 3602, 1114,\n",
       "         3724,  788,  243,    6,    6,    6,  663, 1647, 3682, 3682, 3625,\n",
       "          203, 3008, 3625, 3616,   16, 3599,    4], dtype=int32),\n",
       " memmap([   5, 3676,  848, 3784, 1931,   58, 3676,  416, 2316, 3619, 3625,\n",
       "         3617, 3744, 4335,   12, 3625, 3616,  175, 3662,    7, 3629,  203,\n",
       "            6,    6,    6,    6,    6,    6,  143, 3625, 3616,  131, 3662,\n",
       "          342, 3629, 3616, 3602,  176,  334,  829, 1115, 3665,    6,    6,\n",
       "         3451, 1633,  375,  671, 1644, 3608,  547, 3423,  765,  815, 3604,\n",
       "            6,    6,    6, 2375, 3608, 3604,  532, 2589, 3599,    4,  307,\n",
       "          323,    6,  321, 3611,  622,  122, 3725, 3620, 3627, 3837, 3608,\n",
       "            6,  176,  268, 4082,   94,  567, 4014, 3617, 7474, 3616, 3830,\n",
       "           66, 3590,  307,  192, 1272,  158, 3788,  353, 3599,  202,  316,\n",
       "         3600,  176,   10,  323,  476, 3663, 1329,  605,  238, 3631, 2470,\n",
       "         3604, 1939,  106, 3627,   13,    6,    6, 1128,   48,    6,    6,\n",
       "          848, 3784, 3833,    8, 3637, 2263,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 0,\n",
       " 1,\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  479, 3652, 3625,  243,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  813,   17, 3599,  307,  587,  931,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,   18, 3686,    0,    0,\n",
       "         3324,    0,    0,    0,    0,    0,  207, 3714,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,   49, 3632,  796,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          578, 3652, 3625, 3617, 4148, 3665,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1381, 4148,\n",
       "         3451,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          752, 3608, 3604,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, 2143,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          347,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,  162,  490,    0,    0,   28, 3599,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking first and end\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d3909",
   "metadata": {},
   "source": [
    "## 5. Implementing the BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142cd19",
   "metadata": {},
   "source": [
    "### Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d723a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    Function for calculating the pad mask\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Gelu activation function\n",
    "    :param x: input value\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "\n",
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    Create parameter initializer\n",
    "    :param stddev: Standard deviation of the random variable to be generated\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    Create bias initializer\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "\n",
    "\n",
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    Class for using JSON as a configuration format\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        Create Config from a file\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f3f87",
   "metadata": {},
   "source": [
    "### Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e30154a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param config: Config object\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create shared weight\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        Run the layer\n",
    "        :param inputs: input\n",
    "        :param mode: execution mode\n",
    "        :return: embedding or linear execution result\n",
    "        \"\"\"\n",
    "        # If the mode is 'embedding', execute an embedding lookup.\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # If the mode is 'linear', execute a linear operation.\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # If the mode is other than the specified options, raise an error\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: input\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        Execute the linear\n",
    "        :param inputs: input\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c04f6",
   "metadata": {},
   "source": [
    "### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24189866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param config: Config object\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Execute the layer \n",
    "        :param inputs: input\n",
    "        :return embed: position embedding lookup result\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704056e4",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca239d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        Execute the layer\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: execution mode\n",
    "        :return attn_out: attention execution result\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param config: Config object\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        Execute the layer\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: execution mode\n",
    "        :return attn_out: attention execution result\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m) # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out = tf.transpose(attn_out, [0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out, [batch_size, -1, self.d_model])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce29fd3",
   "metadata": {},
   "source": [
    "### Position wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d63342a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param config: Config object\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Execute the layer\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward execution result\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2362f8",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f61509e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param config: Config object\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        Execute the layer\n",
    "        :param enc_embed: enc_embed or the output of the previous EncoderLayer\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer execution result\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f9525",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e4a4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param config: Config object\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Execute the layer\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: Logits for the next token prediction for 'dec_tokens'\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: input tokens\n",
    "        :param segments: input segments\n",
    "        :return embed: embedding result\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58eac6",
   "metadata": {},
   "source": [
    "### output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d941801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332ba23",
   "metadata": {},
   "source": [
    "### Function for creating a model (model 생성용 함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04887af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac4b2c",
   "metadata": {},
   "source": [
    "### Define the 'config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19f9d75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117994c8",
   "metadata": {},
   "source": [
    "### Check the model training with test data (가짜 데이터로 모델 훈련 체크)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb2444ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 30s 11ms/step - loss: 9.7747 - nsp_loss: 0.7198 - mlm_loss: 9.0549 - nsp_acc: 0.6000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.5973 - nsp_loss: 0.6165 - mlm_loss: 7.9808 - nsp_acc: 0.8000 - mlm_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f861715bca0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test train\n",
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac154b",
   "metadata": {},
   "source": [
    "## 6. Proceed with pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0aa96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Loss calculation function\n",
    "    :param y_true: Ground truth (bs, n_seq)\n",
    "    :param y_pred: Predicted values (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # Calculate the loss\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # Mask the parts with pad(0)\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # Increase it 20 times to better learn mlm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32842d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Accuracy calculation function\n",
    "    :param y_true: Ground truth (bs, n_seq)\n",
    "    :param y_pred: Predicted values (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # Verification of correctness\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # Mask the parts with pad(0)\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # Compute the accuracy\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7eaa375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param train_steps: total sum of training steps\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: maximum of learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        Compute the learning rate\n",
    "        :param step_num: Current step number\n",
    "        :retrun: Calculated learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a56bbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14d34d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 4485632     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 4,551,936\n",
      "Trainable params: 4,551,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a900258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# Compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9592fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 246s 122ms/step - loss: 19.5846 - nsp_loss: 0.6505 - mlm_loss: 18.9341 - nsp_acc: 0.5895 - mlm_lm_acc: 0.1085\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.10852, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 17.5079 - nsp_loss: 0.6220 - mlm_loss: 16.8859 - nsp_acc: 0.6181 - mlm_lm_acc: 0.1298\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.10852 to 0.12976, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 15.9552 - nsp_loss: 0.6156 - mlm_loss: 15.3396 - nsp_acc: 0.6252 - mlm_lm_acc: 0.1493\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.12976 to 0.14933, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 243s 121ms/step - loss: 14.1578 - nsp_loss: 0.6111 - mlm_loss: 13.5467 - nsp_acc: 0.6313 - mlm_lm_acc: 0.1886\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.14933 to 0.18857, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 13.3736 - nsp_loss: 0.6062 - mlm_loss: 12.7674 - nsp_acc: 0.6398 - mlm_lm_acc: 0.2123\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.18857 to 0.21228, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 12.9106 - nsp_loss: 0.6019 - mlm_loss: 12.3087 - nsp_acc: 0.6480 - mlm_lm_acc: 0.2272\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.21228 to 0.22724, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 12.5998 - nsp_loss: 0.5971 - mlm_loss: 12.0027 - nsp_acc: 0.6597 - mlm_lm_acc: 0.2380\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.22724 to 0.23798, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 243s 121ms/step - loss: 12.3913 - nsp_loss: 0.5920 - mlm_loss: 11.7994 - nsp_acc: 0.6693 - mlm_lm_acc: 0.2451\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.23798 to 0.24507, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 12.2664 - nsp_loss: 0.5878 - mlm_loss: 11.6786 - nsp_acc: 0.6765 - mlm_lm_acc: 0.2493\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.24507 to 0.24932, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 12.2091 - nsp_loss: 0.5852 - mlm_loss: 11.6239 - nsp_acc: 0.6816 - mlm_lm_acc: 0.2513\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.24932 to 0.25129, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# Train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbf481",
   "metadata": {},
   "source": [
    "## 7. Project result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "295ed233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "# plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(bottom=0.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "# plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(bottom=0)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(bottom=0, top=1)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
