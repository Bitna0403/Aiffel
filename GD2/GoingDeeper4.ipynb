{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911f6844",
   "metadata": {},
   "source": [
    "# LSA & LDA 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c30831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7cf1f6",
   "metadata": {},
   "source": [
    "NLTK 데이터셋을 다운로드하지 않은 상태라면 아래의 커맨드를 통해 다운로드해준다. NLTK는 데이터셋을 다운로드해 주지 않으면 NLTK의 도구들이 제대로 동작하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6473f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /aiffel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /aiffel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900172a",
   "metadata": {},
   "source": [
    "## 데이터 다운로드 및 확인\n",
    "실습을 위해 데이터를 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b30d2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/aiffel/aiffel/topic_modelling/data/abcnews-date-text.csv',\n",
       " <http.client.HTTPMessage at 0x7fb19a69aaf0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "csv_filename = os.getenv('HOME')+'/aiffel/topic_modelling/data/abcnews-date-text.csv'\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", \n",
    "                           filename=csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1ff78",
   "metadata": {},
   "source": [
    "다운로드한 데이터를 데이터프레임에 저장하고 전체 샘플의 수를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d99f041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082168, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(csv_filename, on_bad_lines='skip')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5547ad0",
   "metadata": {},
   "source": [
    "약 108만 개의 샘플이 존재한다. 5개의 샘플만 출력해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a417c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f79fa",
   "metadata": {},
   "source": [
    "publish_date는 이번 실습에 불필요하므로 headline_text만 별도로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374b1b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[['headline_text']].copy()\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f3bf9",
   "metadata": {},
   "source": [
    "데이터에 중복이 있는지 확인해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a99633e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline_text    1054983\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.nunique() # 중복을 제외하고 유일한 시퀀스를 가지는 샘플의 개수를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec3645",
   "metadata": {},
   "source": [
    "약 108만 개의 샘플 중 중복을 제외하면 약 105만 개의 샘플이 존재하는데 이는 약 3만 개의 샘플이 중복 샘플임을 의미한다.\n",
    "중복 샘플을 제거해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d01fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054983, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.drop_duplicates(inplace=True) # 중복 샘플 제거\n",
    "text.reset_index(drop=True, inplace=True)\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6aa642",
   "metadata": {},
   "source": [
    "## 데이터 정제 및 정규화\n",
    "이제 텍스트 데이터를 정제 및 정규화하는 과정을 진행해 본다.\n",
    "우선 NLTK의 토크나이저를 이용해 전체 텍스트 데이터에 대해서 단어 토큰화를 수행하고, NLTK가 제공하는 불용어 리스트를 사용하여 불용어를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92282354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decides, community, broadcasting, licence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witnesses, must, aware, defamation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[g, calls, infrastructure, protection, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, aust, strike, pay, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, affect, australian, travellers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0   [aba, decides, community, broadcasting, licence]\n",
       "1    [act, fire, witnesses, must, aware, defamation]\n",
       "2     [g, calls, infrastructure, protection, summit]\n",
       "3          [air, nz, staff, aust, strike, pay, rise]\n",
       "4  [air, nz, strike, affect, australian, travellers]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 토크나이저를 이용해서 토큰화\n",
    "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n",
    "\n",
    "# 불용어 제거\n",
    "stop_words = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words)])\n",
    "\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe024889",
   "metadata": {},
   "source": [
    "이제 동일한 단어지만 다른 표현을 가지는 단어들을 하나의 단어로 통합(lemmatization)하는 단어 정규화 과정, 그리고 길이가 1 ~ 2인 단어를 제거하는 전처리를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d45b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [aba, decide, community, broadcast, licence]\n",
      "1    [act, fire, witness, must, aware, defamation]\n",
      "2       [call, infrastructure, protection, summit]\n",
      "3            [air, staff, aust, strike, pay, rise]\n",
      "4    [air, strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 단어 정규화. 3인칭 단수 표현 -> 1인칭 변환, 과거형 동사 -> 현재형 동사 등을 수행한다.\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "\n",
    "# 길이가 1 ~ 2인 단어는 제거.\n",
    "text = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "print(text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756e0d8",
   "metadata": {},
   "source": [
    "## 역토큰화 및 DTM 생성\n",
    "DTM을 생성하는 CountVectorizer 또는 TF-IDF 행렬을 생성하는 TfidfVectorizer의 입력으로 사용하기 위해서 토큰화 과정을 역으로 되돌리는 역토큰화(detokenization) 를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bada5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역토큰화 (토큰화 작업을 역으로 수행)\n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(text[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "train_data = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef5003",
   "metadata": {},
   "source": [
    "전처리 최종 결과는 train_data에 저장했다. 5개의 샘플을 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f3395c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aba decide community broadcast licence',\n",
       " 'act fire witness must aware defamation',\n",
       " 'call infrastructure protection summit',\n",
       " 'air staff aust strike pay rise',\n",
       " 'air strike affect australian travellers']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707452c0",
   "metadata": {},
   "source": [
    "전처리 최종 결과인 train_data는 다음에 배울 LDA 실습에서도 재사용 할 것이다. CountVectorizer를 사용하여 DTM을 생성해보자. 단어의 수는 5,000개로 제한한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2492c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 5000개의 단어만 사용\n",
    "c_vectorizer = CountVectorizer(stop_words='english', max_features = 5000)\n",
    "document_term_matrix = c_vectorizer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712f74b",
   "metadata": {},
   "source": [
    "DTM을 생성했고 크기를 확인해보자.\n",
    "DTM의 크기(shape)는 (문서의 수 × 단어 집합의 크기)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b77747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬의 크기 : (1054983, 5000)\n"
     ]
    }
   ],
   "source": [
    "print('행렬의 크기 :',document_term_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7055a3c",
   "metadata": {},
   "source": [
    "## scikit-learn TruncatedSVD 활용\n",
    "이제 Truncated SVD를 통해 LSA를 수행해 보자. 토픽의 수를 10으로 정한다. 이는 앞서 배운 하이퍼파라미터 k에 해당되며 행렬 Vk^T가 k × (단어의 수)의 크기를 가지도록 DTM에 TruncatedSVD를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d2b37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01204626, -0.00371201,  0.01826968, ...,  0.00497032,\n",
       "         0.00142558,  0.0149266 ],\n",
       "       [ 0.02907276, -0.01096771,  0.01817933, ..., -0.0028364 ,\n",
       "        -0.01029897, -0.00332909],\n",
       "       [ 0.00503577, -0.00203489,  0.00974656, ..., -0.00240685,\n",
       "         0.0012539 ,  0.00374946],\n",
       "       ...,\n",
       "       [ 0.02971409,  0.00417502,  0.0248411 , ...,  0.03553603,\n",
       "         0.01972693,  0.01455705],\n",
       "       [ 0.06174812, -0.00451568,  0.13664588, ...,  0.8418204 ,\n",
       "         0.85542486, -0.4074625 ],\n",
       "       [ 0.07149276,  0.02849891,  0.00177157, ...,  0.01348028,\n",
       "        -0.02249333,  0.02742473]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_topics = 10\n",
    "lsa_model = TruncatedSVD(n_components = n_topics)\n",
    "lsa_model.fit_transform(document_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529258f",
   "metadata": {},
   "source": [
    "TruncatedSVD를 통해 얻은 행렬 Vk^T 크기를 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d0a1d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(lsa_model.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9038cb",
   "metadata": {},
   "source": [
    "행렬 Vk^T가 k × (단어의 수) 의 크기를 가지는 것을 확인할 수 있다. 이제 각 행을 전체 코퍼스의 k개의 주제(topic)로 판단하고 각 주제에서 n개씩 단어를 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96345b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('police', 0.74635), ('man', 0.4535), ('charge', 0.21086), ('new', 0.14089), ('court', 0.11154)]\n",
      "Topic 2: [('man', 0.69421), ('charge', 0.30043), ('court', 0.16825), ('face', 0.11419), ('murder', 0.10645)]\n",
      "Topic 3: [('new', 0.83688), ('plan', 0.23605), ('say', 0.18256), ('govt', 0.1104), ('council', 0.11023)]\n",
      "Topic 4: [('say', 0.73929), ('plan', 0.35756), ('govt', 0.16704), ('council', 0.13123), ('urge', 0.07507)]\n",
      "Topic 5: [('plan', 0.73189), ('council', 0.18094), ('govt', 0.13872), ('urge', 0.08701), ('water', 0.07331)]\n",
      "Topic 6: [('govt', 0.53728), ('court', 0.25548), ('urge', 0.25544), ('fund', 0.21187), ('face', 0.16954)]\n",
      "Topic 7: [('charge', 0.51854), ('court', 0.45861), ('face', 0.35406), ('plan', 0.12071), ('murder', 0.11342)]\n",
      "Topic 8: [('win', 0.55317), ('court', 0.35284), ('kill', 0.2081), ('crash', 0.17374), ('australia', 0.09471)]\n",
      "Topic 9: [('win', 0.58161), ('charge', 0.49313), ('australia', 0.09669), ('cup', 0.07553), ('world', 0.06914)]\n",
      "Topic 10: [('council', 0.71164), ('kill', 0.28735), ('crash', 0.17933), ('charge', 0.14651), ('fund', 0.12357)]\n"
     ]
    }
   ],
   "source": [
    "terms = c_vectorizer.get_feature_names_out() # 단어 집합. 5,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(lsa_model.components_, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f791c",
   "metadata": {},
   "source": [
    "# LDA\n",
    "## TF-IDF 행렬 생성\n",
    "LDA는 DTM 또는 TF-IDF를 입력으로 받을 수 있다. 여기서는 TF-IDF를 사용한다.\n",
    "\n",
    "TfidfVectorizer를 사용하여 TF-IDF 행렬을 생성해보자. 단어의 수는 5,000개로 제한한다. TF-IDF 행렬을 생성한 후에는 행렬의 크기를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8db48f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬의 크기 : (1054983, 5000)\n"
     ]
    }
   ],
   "source": [
    "# 상위 5,000개의 단어만 사용\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tf_idf_matrix = tfidf_vectorizer.fit_transform(train_data)\n",
    "\n",
    "# TF-IDF 행렬의 크기를 확인해봅시다.\n",
    "print('행렬의 크기 :', tf_idf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949bc887",
   "metadata": {},
   "source": [
    "## scikit-learn LDA Model 활용\n",
    "사이킷런의 LDA 모델을 사용하여 학습한다. LSA와 마찬가지로 동일한 사이킷런 패키지이므로 앞으로 진행되는 실습 과정은 LSA와 매우 유사하다. 토픽의 개수는 10개로 정했는데 이는 n_components의 인자값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9dcfd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0335099 , 0.0335099 , 0.0335099 , ..., 0.17024867, 0.0335099 ,\n",
       "        0.0335099 ],\n",
       "       [0.03365631, 0.03365631, 0.03365631, ..., 0.03365631, 0.03365631,\n",
       "        0.03365631],\n",
       "       [0.25184095, 0.0366096 , 0.0366096 , ..., 0.0366096 , 0.0366096 ,\n",
       "        0.0366096 ],\n",
       "       ...,\n",
       "       [0.26687206, 0.02914502, 0.02914502, ..., 0.13007484, 0.02916018,\n",
       "        0.28739608],\n",
       "       [0.10378115, 0.02637829, 0.12325014, ..., 0.02637829, 0.02637829,\n",
       "        0.02637829],\n",
       "       [0.03376055, 0.03376055, 0.2255442 , ..., 0.03376055, 0.03376055,\n",
       "        0.03376055]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', random_state=777, max_iter=1)\n",
    "lda_model.fit_transform(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08ced6",
   "metadata": {},
   "source": [
    "LDA를 통해 얻은 결과 행렬의 크기를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "883f5954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2d4d8",
   "metadata": {},
   "source": [
    "전체 코퍼스로부터 얻은 10개의 토픽과 각 토픽에서의 단어의 비중을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2235a26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('australia', 9359.06334), ('sydney', 5854.97288), ('attack', 4784.76322), ('change', 4193.63035), ('year', 3924.88997)]\n",
      "Topic 2: [('government', 6344.07413), ('charge', 5947.12292), ('man', 4519.7974), ('state', 3658.16422), ('live', 3625.10473)]\n",
      "Topic 3: [('australian', 7666.65651), ('say', 7561.01807), ('police', 5513.22932), ('home', 4048.38409), ('report', 3796.04446)]\n",
      "Topic 4: [('melbourne', 5298.35047), ('south', 4844.59835), ('death', 4281.78433), ('china', 3214.44581), ('women', 3029.28443)]\n",
      "Topic 5: [('win', 5704.0914), ('canberra', 4322.0963), ('die', 4025.63057), ('open', 3771.65243), ('warn', 3577.47151)]\n",
      "Topic 6: [('court', 5246.3124), ('world', 4536.86331), ('country', 4166.34794), ('woman', 3983.97748), ('crash', 3793.50267)]\n",
      "Topic 7: [('election', 5418.5038), ('adelaide', 4864.95604), ('house', 4478.6135), ('school', 3966.82676), ('2016', 3955.11155)]\n",
      "Topic 8: [('trump', 8189.58575), ('new', 6625.2724), ('north', 3705.40987), ('rural', 3521.42659), ('donald', 3356.26657)]\n",
      "Topic 9: [('perth', 4552.8151), ('kill', 4093.61782), ('break', 2695.71958), ('budget', 2596.93268), ('children', 2586.01957)]\n",
      "Topic 10: [('queensland', 5552.68506), ('coast', 3825.32603), ('tasmanian', 3550.75997), ('shoot', 3185.71575), ('service', 2695.21462)]\n"
     ]
    }
   ],
   "source": [
    "# LDA의 결과 토픽과 각 단어의 비중을 출력합시다\n",
    "terms = tfidf_vectorizer.get_feature_names_out() # 단어 집합. 5,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n-1:-1]])\n",
    "\n",
    "get_topics(lda_model.components_, terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
