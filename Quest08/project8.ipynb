{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb02249b",
   "metadata": {},
   "source": [
    "평가문항 상세기준\n",
    "\n",
    "1. 한국어 전처리를 통해 학습 데이터셋을 구축하였다.\t공백과 특수문자 처리, 토크나이징, 병렬데이터 구축의 과정이 적절히 진행되었다.\n",
    "\n",
    "2. 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다.\t구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적으로 수렴하였다.\n",
    "\n",
    "3. 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다. 한국어 입력문장에 맥락에 맞는 한국어로 답변을 리턴하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1e8979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738dcfb",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7effe152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.getenv('HOME')+'/aiffel/transformer_chatbot/data/'\n",
    "csv_path = os.path.join(data_path, 'ChatbotData .csv')\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f9989d",
   "metadata": {},
   "source": [
    "전체 데이터는 11823개이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ce9d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n"
     ]
    }
   ],
   "source": [
    "# 사용할 샘플의 최대 개수\n",
    "MAX_SAMPLES = len(data)\n",
    "print(MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6813d",
   "metadata": {},
   "source": [
    "랜덤으로 10개의 질문과 답변을 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0624e95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : 12시 땡!\n",
      "A : 하루가 또 가네요.\n",
      "\n",
      "Q : 1지망 학교 떨어졌어\n",
      "A : 위로해 드립니다.\n",
      "\n",
      "Q : 3박4일 놀러가고 싶다\n",
      "A : 여행은 언제나 좋죠.\n",
      "\n",
      "Q : 3박4일 정도 놀러가고 싶다\n",
      "A : 여행은 언제나 좋죠.\n",
      "\n",
      "Q : PPL 심하네\n",
      "A : 눈살이 찌푸려지죠.\n",
      "\n",
      "Q : SD카드 망가졌어\n",
      "A : 다시 새로 사는 게 마음 편해요.\n",
      "\n",
      "Q : SD카드 안돼\n",
      "A : 다시 새로 사는 게 마음 편해요.\n",
      "\n",
      "Q : SNS 맞팔 왜 안하지ㅠㅠ\n",
      "A : 잘 모르고 있을 수도 있어요.\n",
      "\n",
      "Q : SNS 시간낭비인 거 아는데 매일 하는 중\n",
      "A : 시간을 정하고 해보세요.\n",
      "\n",
      "Q : SNS 시간낭비인데 자꾸 보게됨\n",
      "A : 시간을 정하고 해보세요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Q :', data['Q'][i])\n",
    "    print('A :', data['A'][i])    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6c97c",
   "metadata": {},
   "source": [
    "## 중복 데이터 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d7661",
   "metadata": {},
   "source": [
    "Q에 있는 중복되는 값이 있는 지 확인한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15895e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>고양이 키우고 싶어</td>\n",
       "      <td>가족들과 상의해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>공시 준비 힘들어</td>\n",
       "      <td>잘 될 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>돈 벌고 싶어</td>\n",
       "      <td>많이 벌수록 좋아요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>로또 번호 알려줘</td>\n",
       "      <td>알면 제가 하죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>마음이 울적해</td>\n",
       "      <td>거리를 걸어보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11642</th>\n",
       "      <td>착해서 잘해주는 건지 좋아하는 건지</td>\n",
       "      <td>헷갈린다고 말해보세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11649</th>\n",
       "      <td>첫 눈에 반하는게 가능해?</td>\n",
       "      <td>당연히 가능하죠.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11658</th>\n",
       "      <td>첫사랑 생각나</td>\n",
       "      <td>지금의 사랑에 충실하세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11732</th>\n",
       "      <td>커플여행이 나을까 그냥 우리끼리 갈까?</td>\n",
       "      <td>저는 둘이 가는 게 좋아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Q                A  label\n",
       "196               고양이 키우고 싶어     가족들과 상의해보세요.      0\n",
       "235                공시 준비 힘들어         잘 될 거예요.      0\n",
       "1294                 돈 벌고 싶어      많이 벌수록 좋아요.      0\n",
       "1445               로또 번호 알려줘        알면 제가 하죠.      0\n",
       "1481                 마음이 울적해       거리를 걸어보세요.      0\n",
       "...                      ...              ...    ...\n",
       "11642    착해서 잘해주는 건지 좋아하는 건지     헷갈린다고 말해보세요.      2\n",
       "11649         첫 눈에 반하는게 가능해?        당연히 가능하죠.      2\n",
       "11658                첫사랑 생각나   지금의 사랑에 충실하세요.      2\n",
       "11732  커플여행이 나을까 그냥 우리끼리 갈까?  저는 둘이 가는 게 좋아요.      2\n",
       "11819         훔쳐보는 것도 눈치 보임.    훔쳐보는 거 티나나봐요.      2\n",
       "\n",
       "[161 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.duplicated(['Q'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3bc52f",
   "metadata": {},
   "source": [
    "1481번의 '마음이 울적해'와 중복되는 것이 1480번에도 있는 것을 알 수 있다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d92f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>마음이 울적해</td>\n",
       "      <td>소리를 크게한번 질러보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>마음이 울적해</td>\n",
       "      <td>거리를 걸어보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Q                A  label\n",
       "1480  마음이 울적해  소리를 크게한번 질러보세요.      0\n",
       "1481  마음이 울적해       거리를 걸어보세요.      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Q']==data['Q'][1481]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bbe14b",
   "metadata": {},
   "source": [
    "중복되는 데이터들을 제거해주니 7727개의 데이터가 남았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383dd8da",
   "metadata": {},
   "source": [
    "A에도 중복되는 데이터가 있는 지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e357f1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SD카드 안돼</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>가끔 뭐하는지 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>가스불 켜놓고 나온거 같아</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11806</th>\n",
       "      <td>혼자만 설레고 혼자서 끝내는 짝사랑 그만할래.</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11807</th>\n",
       "      <td>화이트데이에 고백할까요?</td>\n",
       "      <td>선물을 주면서 솔직하고 당당하게 고백해보세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11809</th>\n",
       "      <td>확실히 날 좋아하는 걸 아는 남자랑 친구가 될 수 있을까?</td>\n",
       "      <td>그 사람을 위해서는 그러면 안돼요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11816</th>\n",
       "      <td>회식하는데 나만 챙겨줘. 썸임?</td>\n",
       "      <td>호감이 있을 수도 있어요. 그렇지만 조금 더 상황을 지켜보세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4044 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Q                                    A  \\\n",
       "3                       3박4일 정도 놀러가고 싶다                          여행은 언제나 좋죠.   \n",
       "6                               SD카드 안돼                   다시 새로 사는 게 마음 편해요.   \n",
       "9                     SNS 시간낭비인데 자꾸 보게됨                        시간을 정하고 해보세요.   \n",
       "12                          가끔 뭐하는지 궁금해                        그 사람도 그럴 거예요.   \n",
       "18                       가스불 켜놓고 나온거 같아                  빨리 집에 돌아가서 끄고 나오세요.   \n",
       "...                                 ...                                  ...   \n",
       "11806         혼자만 설레고 혼자서 끝내는 짝사랑 그만할래.                            맘고생 많았어요.   \n",
       "11807                     화이트데이에 고백할까요?            선물을 주면서 솔직하고 당당하게 고백해보세요.   \n",
       "11809  확실히 날 좋아하는 걸 아는 남자랑 친구가 될 수 있을까?                  그 사람을 위해서는 그러면 안돼요.   \n",
       "11816                 회식하는데 나만 챙겨줘. 썸임?  호감이 있을 수도 있어요. 그렇지만 조금 더 상황을 지켜보세요.   \n",
       "11822                        힘들어서 결혼할까봐                   도피성 결혼은 하지 않길 바라요.   \n",
       "\n",
       "       label  \n",
       "3          0  \n",
       "6          0  \n",
       "9          0  \n",
       "12         0  \n",
       "18         0  \n",
       "...      ...  \n",
       "11806      2  \n",
       "11807      2  \n",
       "11809      2  \n",
       "11816      2  \n",
       "11822      2  \n",
       "\n",
       "[4044 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.duplicated(['A'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b53d9",
   "metadata": {},
   "source": [
    "'맘고생 많았어요'와 중복되는 결과들이 엄청 많이 나온다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0345e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5751</th>\n",
       "      <td>끝이났네</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>니가 원망스러워</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6455</th>\n",
       "      <td>방금 헤어지고 왔습니다</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7726</th>\n",
       "      <td>이제는 정말 끝</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8015</th>\n",
       "      <td>정리가 되네</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8211</th>\n",
       "      <td>진짜 끝</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8221</th>\n",
       "      <td>진짜 이제 안녕</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8234</th>\n",
       "      <td>질질 끄는연애 드디어끝이네.</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10303</th>\n",
       "      <td>썸남이 짝남 됐어요. 위로해 주세요.</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11806</th>\n",
       "      <td>혼자만 설레고 혼자서 끝내는 짝사랑 그만할래.</td>\n",
       "      <td>맘고생 많았어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Q          A  label\n",
       "5751                        끝이났네  맘고생 많았어요.      1\n",
       "6010                    니가 원망스러워  맘고생 많았어요.      1\n",
       "6455                방금 헤어지고 왔습니다  맘고생 많았어요.      1\n",
       "7726                    이제는 정말 끝  맘고생 많았어요.      1\n",
       "8015                      정리가 되네  맘고생 많았어요.      1\n",
       "8211                        진짜 끝  맘고생 많았어요.      1\n",
       "8221                    진짜 이제 안녕  맘고생 많았어요.      1\n",
       "8234             질질 끄는연애 드디어끝이네.  맘고생 많았어요.      1\n",
       "10303       썸남이 짝남 됐어요. 위로해 주세요.  맘고생 많았어요.      2\n",
       "11806  혼자만 설레고 혼자서 끝내는 짝사랑 그만할래.  맘고생 많았어요.      2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['A']==data['A'][11806]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f97e9",
   "metadata": {},
   "source": [
    "A에서도 중복 데이터를 제거해주고 나면 7779개가 남게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5ac0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q        0\n",
       "A        0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b900e932",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebdcb15",
   "metadata": {},
   "source": [
    "단어 토크나이징 방해되지 않도록 정규 표현식으로 구두점을 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ffc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "  # student와 온점 사이에 거리를 만듭니다.\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # (가-힣, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "  sentence = re.sub(r\"[^가-힣?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57965d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
    "def load_conversations():\n",
    "  inputs, outputs = [], []\n",
    "  for i in range(MAX_SAMPLES):\n",
    "    # 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용.\n",
    "    inputs.append(preprocess_sentence(data['Q'].values[i]))\n",
    "    outputs.append(preprocess_sentence(data['A'].values[i]))\n",
    "\n",
    "  return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aaec1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 질문 수 : 11823\n",
      "전체 답변 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
    "questions, answers = load_conversations()\n",
    "print('전체 질문 수 :', len(questions))\n",
    "print('전체 답변 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed82e2",
   "metadata": {},
   "source": [
    "위에서 확인한 데이터대로 잘 저장되었다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "761bed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: 마음이 울적해\n",
      "전처리 후의 22번째 답변 샘플: 거리를 걸어보세요 .\n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[1481]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[1481]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bfe01cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31fd476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07460f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8127]\n",
      "END_TOKEN의 번호 : [8128]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee6032cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8129\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b543a",
   "metadata": {},
   "source": [
    "## 각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46a51dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5742, 612, 2481, 4148]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2352, 7481, 7, 6245, 97, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "546c5c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b23941c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "739c88c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8129\n",
      "필터링 후의 질문 샘플 개수: 11823\n",
      "필터링 후의 답변 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72563f",
   "metadata": {},
   "source": [
    "## 교사 강요(Teacher Forcing) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e7d0c",
   "metadata": {},
   "source": [
    "tf.data.Dataset API 는 훈련 프로세스의 속도가 빨라지도록 입력 파이프라인을 구축하는 API이다.\n",
    "\n",
    "이를 적극 사용하기 위해서 질문과 답변의 쌍을 tf.data.Dataset의 입력으로 넣어주는 작업을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae68f436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d291a62",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55103af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "739adf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aedda0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_pos_encoding =  PositionalEncoding(50, 512)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469998b6",
   "metadata": {},
   "source": [
    "## 스케일드 닷 프로덕트 어텐션 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c276cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c58d3",
   "metadata": {},
   "source": [
    "스케일드 닷 프로덕트 어텐션 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "898a43a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84ac08",
   "metadata": {},
   "source": [
    "## 패딩 마스킹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e125e",
   "metadata": {},
   "source": [
    "패딩은 문장의 길이가 서로 다를 때, 모든 문장의 길이를 동일하게 해주는 과정에서 정해준 길이보다 짧은 문장의 경우에는 숫자 0을 채워서 문장의 길이를 맞춰주는 자연어 처리 전처리 방법이다. 그런데 사실 이렇게 주어진 숫자 0은 실제 의미가 있는 단어가 아니므로 실제 어텐션 등과 같은 연산에서는 제외할 필요가 있다. 패딩 마스킹은 이를 위해 숫자 0인 위치를 체크한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96e0706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c765607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 0. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(2, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee02d9",
   "metadata": {},
   "source": [
    "RNN은 step이라는 개념이 존재해서 각 step마다 단어가 순서대로 입력으로 들어가는 구조인 반면 트랜스포머의 경우에는 문장 행렬을 만들어 한 번에 행렬 형태로 입력으로 들어간다는 특징이 있다. 그리고 이 특징 때문에 추가적인 마스킹(Masking) 을 필요한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d21a9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a59cf04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 0. 1. 1. 1.]\n",
      "   [1. 0. 0. 1. 1.]\n",
      "   [1. 0. 0. 0. 1.]\n",
      "   [1. 0. 0. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[1, 2, 3, 4, 5]]))) #숫자 0이 포함되지 않았을 경우\n",
    "print(create_look_ahead_mask(tf.constant([[0, 5, 1, 5, 5]]))) #숫자 0이 포함되었을 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e1cd5",
   "metadata": {},
   "source": [
    "## 인코더 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ce22c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f468d",
   "metadata": {},
   "source": [
    "이렇게 구현한 인코더 층을 임베딩 층(Embedding layer) 과 포지셔널 인코딩(Positional Encoding) 을 연결하고, 사용자가 원하는 만큼 인코더 층을 쌓음으로써 트랜스포머의 인코더가 완성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bc4ca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f566cdc",
   "metadata": {},
   "source": [
    "## 디코더 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4be8a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f52363a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50b57e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f45f99",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73a46e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 500)    8155580     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 500)    10161580    dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8129)   4072629     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 22,389,789\n",
      "Trainable params: 22,389,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 500 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 20 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 1040 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.4 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb2531",
   "metadata": {},
   "source": [
    "## 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1dad6984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d8def",
   "metadata": {},
   "source": [
    "## 커스텀 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afdea16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53c031ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191850f4",
   "metadata": {},
   "source": [
    "## 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eafbfcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40382e",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb60956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "185/185 [==============================] - 25s 102ms/step - loss: 1.3932 - accuracy: 0.0246\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 19s 105ms/step - loss: 1.0987 - accuracy: 0.0485\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 20s 106ms/step - loss: 1.0009 - accuracy: 0.0503\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.9532 - accuracy: 0.0523\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.9148 - accuracy: 0.0549\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.8768 - accuracy: 0.0569\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 19s 105ms/step - loss: 0.8342 - accuracy: 0.0592\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 19s 105ms/step - loss: 0.7862 - accuracy: 0.0623\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.7332 - accuracy: 0.0666\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 19s 105ms/step - loss: 0.6761 - accuracy: 0.0716\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 19s 105ms/step - loss: 0.6171 - accuracy: 0.0778\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 19s 105ms/step - loss: 0.5578 - accuracy: 0.0843\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.5022 - accuracy: 0.0908\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.4506 - accuracy: 0.0972\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 19s 105ms/step - loss: 0.4036 - accuracy: 0.1033\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.3614 - accuracy: 0.1089\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.3232 - accuracy: 0.1140\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.2881 - accuracy: 0.1189\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.2584 - accuracy: 0.1233\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.2299 - accuracy: 0.1281\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.2079 - accuracy: 0.1312\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1866 - accuracy: 0.1346\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1673 - accuracy: 0.1378\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1473 - accuracy: 0.1411\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1292 - accuracy: 0.1447\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1142 - accuracy: 0.1474\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1026 - accuracy: 0.1499\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0915 - accuracy: 0.1522\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0817 - accuracy: 0.1543\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0737 - accuracy: 0.1560\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0666 - accuracy: 0.1576\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0606 - accuracy: 0.1592\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0556 - accuracy: 0.1604\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0514 - accuracy: 0.1615\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0481 - accuracy: 0.1621\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0442 - accuracy: 0.1632\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0417 - accuracy: 0.1639\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0392 - accuracy: 0.1646\n",
      "Epoch 39/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0371 - accuracy: 0.1652\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0344 - accuracy: 0.1657\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0331 - accuracy: 0.1662\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0312 - accuracy: 0.1666\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0300 - accuracy: 0.1669\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0282 - accuracy: 0.1674\n",
      "Epoch 45/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0264 - accuracy: 0.1679\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0253 - accuracy: 0.1683\n",
      "Epoch 47/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0248 - accuracy: 0.1683\n",
      "Epoch 48/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0234 - accuracy: 0.1686\n",
      "Epoch 49/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0221 - accuracy: 0.1691\n",
      "Epoch 50/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0221 - accuracy: 0.1691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc32cd9e340>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c5266",
   "metadata": {},
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00a05fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4524c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aab45709",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question_list = [\n",
    "    \"안녕 오랜만이야\",\n",
    "    \"잘 지냈어?\",\n",
    "    \"널 만나서 기뻐\",\n",
    "    \"뭐 먹을까\",\n",
    "    \"메뉴 추천해줘\",\n",
    "    \"3박4일 놀러가고 싶다\",\n",
    "    \"나랑 놀자\",\n",
    "    \"SD카드 망가졌어\",\n",
    "    \"로또 번호 알려줘\",\n",
    "    \"SNS 시간낭비인 거 아는데 매일 하는 중\",\n",
    "    \"고양이 키우고 싶어\",\n",
    "    \"첫사랑 생각나\"\n",
    "    \"웃어줄래\",\n",
    "    \"삶은 뭘까?\",\n",
    "    \"인생살이 왜이리 힘드냐\",\n",
    "    \"이제 그만 쉬고 싶어\",\n",
    "    \"너무 고독하다\",\n",
    "    \"도망가고 싶다\",\n",
    "    \"넌 최고였어\",\n",
    "    \"나 잊지마\",\n",
    "    \"우리 마지막이야\",\n",
    "    \"나 간다 잘 지내 안녕\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b12dbe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_answers(input_question_list):\n",
    "    for input_qusetion in input_question_list:\n",
    "        sentence_generation(input_qusetion)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c6d03",
   "metadata": {},
   "source": [
    "결과가... 좋은 건지 나쁜 건지 알수가 없지만 마음에 들지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa0c657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕 오랜만이야\n",
      "출력 : 언젠간 올 이별이었을까요 .\n",
      "\n",
      "입력 : 잘 지냈어?\n",
      "출력 : 안부를 물어주시다니 감사합니다 .\n",
      "\n",
      "입력 : 널 만나서 기뻐\n",
      "출력 : 잊어버리세요 .\n",
      "\n",
      "입력 : 뭐 먹을까\n",
      "출력 : 좀 먹어도 괜찮아요 .\n",
      "\n",
      "입력 : 메뉴 추천해줘\n",
      "출력 : 냉장고 파먹기 해보세요 .\n",
      "\n",
      "입력 : 3박4일 놀러가고 싶다\n",
      "출력 : 여행은 언제나 좋죠 .\n",
      "\n",
      "입력 : 나랑 놀자\n",
      "출력 : 지금 그러고 있어요 .\n",
      "\n",
      "입력 : SD카드 망가졌어\n",
      "출력 : 다시 새로 사는 게 마음 편해요 .\n",
      "\n",
      "입력 : 로또 번호 알려줘\n",
      "출력 : 알면 제가 하죠 .\n",
      "\n",
      "입력 : SNS 시간낭비인 거 아는데 매일 하는 중\n",
      "출력 : 시간을 정하고 해보세요 .\n",
      "\n",
      "입력 : 고양이 키우고 싶어\n",
      "출력 : 자신을 먼저 키우세요 .\n",
      "\n",
      "입력 : 첫사랑 생각나웃어줄래\n",
      "출력 : 기대를 내려놓으면 마음이 좀 더 편안해질 거예요 .\n",
      "\n",
      "입력 : 삶은 뭘까?\n",
      "출력 : 삶 곳곳에 사랑이 숨 쉬고 있어요 .\n",
      "\n",
      "입력 : 인생살이 왜이리 힘드냐\n",
      "출력 : 인생을 알아가고 있군요 .\n",
      "\n",
      "입력 : 이제 그만 쉬고 싶어\n",
      "출력 : 잠깐 바람 쐬고 오세요 .\n",
      "\n",
      "입력 : 너무 고독하다\n",
      "출력 : 혼자가 아니에요 .\n",
      "\n",
      "입력 : 도망가고 싶다\n",
      "출력 : 지금도 충분해요 .\n",
      "\n",
      "입력 : 넌 최고였어\n",
      "출력 : 당신도 행복하세요 .\n",
      "\n",
      "입력 : 나 잊지마\n",
      "출력 : 때론 잊어버리는 것이 좋을 때도 있어요 .\n",
      "\n",
      "입력 : 우리 마지막이야\n",
      "출력 : 마지막이 아닐 지도 몰라요 .\n",
      "\n",
      "입력 : 나 간다 잘 지내 안녕\n",
      "출력 : 잘 지냈답니다 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_answers(input_question_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb24f5",
   "metadata": {},
   "source": [
    "에폭 50으로 학습시켰지만 91에서 accuracy가 변동이 계속 있어서 100으로 더 돌려봤다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbb94df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0206 - accuracy: 0.1694\n",
      "Epoch 2/100\n",
      "185/185 [==============================] - 19s 105ms/step - loss: 0.0202 - accuracy: 0.1696\n",
      "Epoch 3/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0191 - accuracy: 0.1699\n",
      "Epoch 4/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0187 - accuracy: 0.1700\n",
      "Epoch 5/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0184 - accuracy: 0.1699\n",
      "Epoch 6/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0176 - accuracy: 0.1703\n",
      "Epoch 7/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0170 - accuracy: 0.1704\n",
      "Epoch 8/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0163 - accuracy: 0.1706\n",
      "Epoch 9/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0163 - accuracy: 0.1705\n",
      "Epoch 10/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0152 - accuracy: 0.1710\n",
      "Epoch 11/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0149 - accuracy: 0.1710\n",
      "Epoch 12/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0142 - accuracy: 0.1711\n",
      "Epoch 13/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0144 - accuracy: 0.1711\n",
      "Epoch 14/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0137 - accuracy: 0.1714\n",
      "Epoch 15/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0138 - accuracy: 0.1712\n",
      "Epoch 16/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0132 - accuracy: 0.1715\n",
      "Epoch 17/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0128 - accuracy: 0.1716\n",
      "Epoch 18/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0124 - accuracy: 0.1717\n",
      "Epoch 19/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0127 - accuracy: 0.1716\n",
      "Epoch 20/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0118 - accuracy: 0.1718\n",
      "Epoch 21/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0119 - accuracy: 0.1717\n",
      "Epoch 22/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0114 - accuracy: 0.1719\n",
      "Epoch 23/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0112 - accuracy: 0.1721\n",
      "Epoch 24/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0110 - accuracy: 0.1720\n",
      "Epoch 25/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0103 - accuracy: 0.1723\n",
      "Epoch 26/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0106 - accuracy: 0.1723\n",
      "Epoch 27/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0102 - accuracy: 0.1723\n",
      "Epoch 28/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0102 - accuracy: 0.1723\n",
      "Epoch 29/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0101 - accuracy: 0.1722\n",
      "Epoch 30/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0095 - accuracy: 0.1723\n",
      "Epoch 31/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0093 - accuracy: 0.1726\n",
      "Epoch 32/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0096 - accuracy: 0.1724\n",
      "Epoch 33/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0092 - accuracy: 0.1725\n",
      "Epoch 34/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0093 - accuracy: 0.1725\n",
      "Epoch 35/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0087 - accuracy: 0.1727\n",
      "Epoch 36/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0087 - accuracy: 0.1727\n",
      "Epoch 37/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0082 - accuracy: 0.1728\n",
      "Epoch 38/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0082 - accuracy: 0.1729\n",
      "Epoch 39/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0081 - accuracy: 0.1728\n",
      "Epoch 40/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0081 - accuracy: 0.1728\n",
      "Epoch 41/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0079 - accuracy: 0.1729\n",
      "Epoch 42/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0081 - accuracy: 0.1728\n",
      "Epoch 43/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0075 - accuracy: 0.1730\n",
      "Epoch 44/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0078 - accuracy: 0.1729\n",
      "Epoch 45/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0075 - accuracy: 0.1729\n",
      "Epoch 46/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0074 - accuracy: 0.1729\n",
      "Epoch 47/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0072 - accuracy: 0.1730\n",
      "Epoch 48/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0075 - accuracy: 0.1730\n",
      "Epoch 49/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0071 - accuracy: 0.1731\n",
      "Epoch 50/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0068 - accuracy: 0.1732\n",
      "Epoch 51/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0069 - accuracy: 0.1732\n",
      "Epoch 52/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0067 - accuracy: 0.1732\n",
      "Epoch 53/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0064 - accuracy: 0.1732\n",
      "Epoch 54/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0066 - accuracy: 0.1733\n",
      "Epoch 55/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0067 - accuracy: 0.1732\n",
      "Epoch 56/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0064 - accuracy: 0.1733\n",
      "Epoch 57/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0063 - accuracy: 0.1732\n",
      "Epoch 58/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0063 - accuracy: 0.1732\n",
      "Epoch 59/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0065 - accuracy: 0.1732\n",
      "Epoch 60/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0059 - accuracy: 0.1733\n",
      "Epoch 61/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0058 - accuracy: 0.1734\n",
      "Epoch 62/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0059 - accuracy: 0.1734\n",
      "Epoch 63/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0061 - accuracy: 0.1733\n",
      "Epoch 64/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0059 - accuracy: 0.1734\n",
      "Epoch 65/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0059 - accuracy: 0.1735\n",
      "Epoch 66/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0057 - accuracy: 0.1734\n",
      "Epoch 67/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0056 - accuracy: 0.1735\n",
      "Epoch 68/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0055 - accuracy: 0.1735\n",
      "Epoch 69/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0054 - accuracy: 0.1735\n",
      "Epoch 70/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0055 - accuracy: 0.1735\n",
      "Epoch 71/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0053 - accuracy: 0.1735\n",
      "Epoch 72/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0052 - accuracy: 0.1736\n",
      "Epoch 73/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0053 - accuracy: 0.1735\n",
      "Epoch 74/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0053 - accuracy: 0.1735\n",
      "Epoch 75/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0051 - accuracy: 0.1736\n",
      "Epoch 76/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0051 - accuracy: 0.1735\n",
      "Epoch 77/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0052 - accuracy: 0.1736\n",
      "Epoch 78/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0052 - accuracy: 0.1735\n",
      "Epoch 79/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0048 - accuracy: 0.1736\n",
      "Epoch 80/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0048 - accuracy: 0.1737\n",
      "Epoch 81/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0049 - accuracy: 0.1736\n",
      "Epoch 82/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0046 - accuracy: 0.1737\n",
      "Epoch 83/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0046 - accuracy: 0.1737\n",
      "Epoch 84/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0047 - accuracy: 0.1737\n",
      "Epoch 85/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0048 - accuracy: 0.1736\n",
      "Epoch 86/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0043 - accuracy: 0.1738\n",
      "Epoch 87/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0046 - accuracy: 0.1737\n",
      "Epoch 88/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0046 - accuracy: 0.1737\n",
      "Epoch 89/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0046 - accuracy: 0.1737\n",
      "Epoch 90/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0044 - accuracy: 0.1738\n",
      "Epoch 91/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0045 - accuracy: 0.1737\n",
      "Epoch 92/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0044 - accuracy: 0.1738\n",
      "Epoch 93/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0044 - accuracy: 0.1738\n",
      "Epoch 94/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0041 - accuracy: 0.1738\n",
      "Epoch 95/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0043 - accuracy: 0.1738\n",
      "Epoch 96/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0042 - accuracy: 0.1738\n",
      "Epoch 97/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0043 - accuracy: 0.1738\n",
      "Epoch 98/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0042 - accuracy: 0.1738\n",
      "Epoch 99/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0041 - accuracy: 0.1738\n",
      "Epoch 100/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0043 - accuracy: 0.1737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc327000ca0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a73fec",
   "metadata": {},
   "source": [
    "오 accuracy가 높아져서 그런지 에폭을 50번 했을때보단 답변이 나아졌다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "911553c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕 오랜만이야\n",
      "출력 : 오랜만이에요 .\n",
      "\n",
      "입력 : 잘 지냈어?\n",
      "출력 : 안부를 물어주시다니 감사합니다 .\n",
      "\n",
      "입력 : 널 만나서 기뻐\n",
      "출력 : 운동 해보는 것도 좋을 거예요 .\n",
      "\n",
      "입력 : 뭐 먹을까\n",
      "출력 : 좀 먹어도 괜찮아요 .\n",
      "\n",
      "입력 : 메뉴 추천해줘\n",
      "출력 : 복스럽게 먹으면 더 좋죠 .\n",
      "\n",
      "입력 : 3박4일 놀러가고 싶다\n",
      "출력 : 여행은 언제나 좋죠 .\n",
      "\n",
      "입력 : 나랑 놀자\n",
      "출력 : 지금 그러고 있어요 .\n",
      "\n",
      "입력 : SD카드 망가졌어\n",
      "출력 : 다시 새로 사는 게 마음 편해요 .\n",
      "\n",
      "입력 : 로또 번호 알려줘\n",
      "출력 : 제가 알면 제가 먼저 했을 거예요 .\n",
      "\n",
      "입력 : SNS 시간낭비인 거 아는데 매일 하는 중\n",
      "출력 : 시간을 정하고 해보세요 .\n",
      "\n",
      "입력 : 고양이 키우고 싶어\n",
      "출력 : 가족들과 상의해보세요 .\n",
      "\n",
      "입력 : 첫사랑 생각나웃어줄래\n",
      "출력 : 기대를 버려야 덜 다칠 거예요 .\n",
      "\n",
      "입력 : 삶은 뭘까?\n",
      "출력 : 삶 곳곳에 사랑이 숨 쉬고 있어요 .\n",
      "\n",
      "입력 : 인생살이 왜이리 힘드냐\n",
      "출력 : 미련이 생기기 전에 그만두는 편이 좋을거예요 .\n",
      "\n",
      "입력 : 이제 그만 쉬고 싶어\n",
      "출력 : 잠깐 바람 쐬고 오세요 .\n",
      "\n",
      "입력 : 너무 고독하다\n",
      "출력 : 혼자가 아니에요 .\n",
      "\n",
      "입력 : 도망가고 싶다\n",
      "출력 : 지금도 충분히 잘 하고 있어요 .\n",
      "\n",
      "입력 : 넌 최고였어\n",
      "출력 : 당신도 행복하세요 .\n",
      "\n",
      "입력 : 나 잊지마\n",
      "출력 : 때론 잊어버리는 것이 좋을 때도 있어요 .\n",
      "\n",
      "입력 : 우리 마지막이야\n",
      "출력 : 마지막이 아닐 지도 몰라요 .\n",
      "\n",
      "입력 : 나 간다 잘 지내 안녕\n",
      "출력 : 잘 지냈답니다 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_answers(input_question_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3a357",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터를 변경한 모델로 다시 학습 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bff9132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 500)    8155580     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 500)    10161580    dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8129)   4072629     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 22,389,789\n",
      "Trainable params: 22,389,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 유지\n",
    "D_MODEL = 500 # 256 -> 500 변경\n",
    "NUM_HEADS = 20 # 8 -> 20 변경 \n",
    "UNITS = 1040 # 512 -> 1040 변경\n",
    "DROPOUT = 0.4 # 0.1 -> 0.4 변경\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "137ce8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bfe221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.7, beta_2=0.8, epsilon=1e-9) #beta_1 0.9 -> 0.7 변경, beta_2 0.98 -> 0.8 변경 \n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f719569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21165fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "185/185 [==============================] - 25s 104ms/step - loss: 1.3971 - accuracy: 0.0246\n",
      "Epoch 2/100\n",
      "185/185 [==============================] - 19s 105ms/step - loss: 1.1004 - accuracy: 0.0484\n",
      "Epoch 3/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 1.0016 - accuracy: 0.0504\n",
      "Epoch 4/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.9546 - accuracy: 0.0526\n",
      "Epoch 5/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.9157 - accuracy: 0.0549\n",
      "Epoch 6/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.8783 - accuracy: 0.0569\n",
      "Epoch 7/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.8364 - accuracy: 0.0592\n",
      "Epoch 8/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.7893 - accuracy: 0.0621\n",
      "Epoch 9/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.7363 - accuracy: 0.0663\n",
      "Epoch 10/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.6797 - accuracy: 0.0712\n",
      "Epoch 11/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.6203 - accuracy: 0.0773\n",
      "Epoch 12/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.5606 - accuracy: 0.0837\n",
      "Epoch 13/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.5030 - accuracy: 0.0905\n",
      "Epoch 14/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.4493 - accuracy: 0.0973\n",
      "Epoch 15/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.4011 - accuracy: 0.1036\n",
      "Epoch 16/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.3588 - accuracy: 0.1091\n",
      "Epoch 17/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.3200 - accuracy: 0.1144\n",
      "Epoch 18/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.2865 - accuracy: 0.1191\n",
      "Epoch 19/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.2568 - accuracy: 0.1236\n",
      "Epoch 20/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.2310 - accuracy: 0.1277\n",
      "Epoch 21/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.2088 - accuracy: 0.1313\n",
      "Epoch 22/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1920 - accuracy: 0.1338\n",
      "Epoch 23/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1726 - accuracy: 0.1369\n",
      "Epoch 24/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1532 - accuracy: 0.1403\n",
      "Epoch 25/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1362 - accuracy: 0.1434\n",
      "Epoch 26/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1222 - accuracy: 0.1460\n",
      "Epoch 27/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.1093 - accuracy: 0.1484\n",
      "Epoch 28/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0973 - accuracy: 0.1509\n",
      "Epoch 29/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0879 - accuracy: 0.1527\n",
      "Epoch 30/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0798 - accuracy: 0.1547\n",
      "Epoch 31/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0716 - accuracy: 0.1566\n",
      "Epoch 32/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0668 - accuracy: 0.1577\n",
      "Epoch 33/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0611 - accuracy: 0.1591\n",
      "Epoch 34/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0563 - accuracy: 0.1601\n",
      "Epoch 35/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0515 - accuracy: 0.1614\n",
      "Epoch 36/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0479 - accuracy: 0.1624\n",
      "Epoch 37/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0461 - accuracy: 0.1627\n",
      "Epoch 38/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0427 - accuracy: 0.1636\n",
      "Epoch 39/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0395 - accuracy: 0.1645\n",
      "Epoch 40/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0378 - accuracy: 0.1647\n",
      "Epoch 41/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0349 - accuracy: 0.1655\n",
      "Epoch 42/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0334 - accuracy: 0.1660\n",
      "Epoch 43/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0328 - accuracy: 0.1663\n",
      "Epoch 44/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0308 - accuracy: 0.1668\n",
      "Epoch 45/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0283 - accuracy: 0.1674\n",
      "Epoch 46/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0281 - accuracy: 0.1675\n",
      "Epoch 47/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0261 - accuracy: 0.1680\n",
      "Epoch 48/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0257 - accuracy: 0.1683\n",
      "Epoch 49/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0242 - accuracy: 0.1684\n",
      "Epoch 50/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0236 - accuracy: 0.1686\n",
      "Epoch 51/100\n",
      "185/185 [==============================] - 19s 103ms/step - loss: 0.0224 - accuracy: 0.1688\n",
      "Epoch 52/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0223 - accuracy: 0.1691\n",
      "Epoch 53/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0209 - accuracy: 0.1693\n",
      "Epoch 54/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0197 - accuracy: 0.1697\n",
      "Epoch 55/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0199 - accuracy: 0.1697\n",
      "Epoch 56/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0193 - accuracy: 0.1699\n",
      "Epoch 57/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0183 - accuracy: 0.1701\n",
      "Epoch 58/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0173 - accuracy: 0.1704\n",
      "Epoch 59/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0172 - accuracy: 0.1704\n",
      "Epoch 60/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0165 - accuracy: 0.1705\n",
      "Epoch 61/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0165 - accuracy: 0.1705\n",
      "Epoch 62/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0155 - accuracy: 0.1709\n",
      "Epoch 63/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0156 - accuracy: 0.1707\n",
      "Epoch 64/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0147 - accuracy: 0.1709\n",
      "Epoch 65/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0147 - accuracy: 0.1710\n",
      "Epoch 66/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0140 - accuracy: 0.1714\n",
      "Epoch 67/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0141 - accuracy: 0.1713\n",
      "Epoch 68/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0134 - accuracy: 0.1714\n",
      "Epoch 69/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0127 - accuracy: 0.1715\n",
      "Epoch 70/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0127 - accuracy: 0.1715\n",
      "Epoch 71/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0125 - accuracy: 0.1716\n",
      "Epoch 72/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0127 - accuracy: 0.1716\n",
      "Epoch 73/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0117 - accuracy: 0.1718\n",
      "Epoch 74/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0120 - accuracy: 0.1718\n",
      "Epoch 75/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0111 - accuracy: 0.1720\n",
      "Epoch 76/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0116 - accuracy: 0.1718\n",
      "Epoch 77/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0109 - accuracy: 0.1722\n",
      "Epoch 78/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0109 - accuracy: 0.1721\n",
      "Epoch 79/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0107 - accuracy: 0.1721\n",
      "Epoch 80/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0105 - accuracy: 0.1722\n",
      "Epoch 81/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0103 - accuracy: 0.1723\n",
      "Epoch 82/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0097 - accuracy: 0.1724\n",
      "Epoch 83/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0101 - accuracy: 0.1723\n",
      "Epoch 84/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0098 - accuracy: 0.1724\n",
      "Epoch 85/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0097 - accuracy: 0.1724\n",
      "Epoch 86/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0093 - accuracy: 0.1725\n",
      "Epoch 87/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0090 - accuracy: 0.1726\n",
      "Epoch 88/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0090 - accuracy: 0.1725\n",
      "Epoch 89/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0086 - accuracy: 0.1727\n",
      "Epoch 90/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0088 - accuracy: 0.1726\n",
      "Epoch 91/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0086 - accuracy: 0.1726\n",
      "Epoch 92/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0086 - accuracy: 0.1727\n",
      "Epoch 93/100\n",
      "185/185 [==============================] - 19s 103ms/step - loss: 0.0084 - accuracy: 0.1728\n",
      "Epoch 94/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0082 - accuracy: 0.1728\n",
      "Epoch 95/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0080 - accuracy: 0.1728\n",
      "Epoch 96/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0081 - accuracy: 0.1728\n",
      "Epoch 97/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0076 - accuracy: 0.1729\n",
      "Epoch 98/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0079 - accuracy: 0.1729\n",
      "Epoch 99/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0077 - accuracy: 0.1729\n",
      "Epoch 100/100\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0073 - accuracy: 0.1730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc28de6ed00>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca00d5",
   "metadata": {},
   "source": [
    "처음에 하이퍼파라미터를 이것저것 바꿔가며 학습시켰을 땐 엄청 좋은 결과가 있어서 조금씩 바꾸다가 그 기록을 남기지 못하고 결국 초기보다 더 낮은 accuracy를 갖게 되었다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8fce3a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕 오랜만이야\n",
      "출력 : 오랜만이에요 .\n",
      "\n",
      "입력 : 잘 지냈어?\n",
      "출력 : 안부를 물어주시다니 감사합니다 .\n",
      "\n",
      "입력 : 널 만나서 기뻐\n",
      "출력 : 지금 무슨 말을 해도 와닿지 않겠지만 잘할 수 있을 거예요 .\n",
      "\n",
      "입력 : 뭐 먹을까\n",
      "출력 : 좀 먹어도 괜찮아요 .\n",
      "\n",
      "입력 : 메뉴 추천해줘\n",
      "출력 : 냉장고 파먹기 해보세요 .\n",
      "\n",
      "입력 : 3박4일 놀러가고 싶다\n",
      "출력 : 여행은 언제나 좋죠 .\n",
      "\n",
      "입력 : 나랑 놀자\n",
      "출력 : 지금 그러고 있어요 .\n",
      "\n",
      "입력 : SD카드 망가졌어\n",
      "출력 : 다시 새로 사는 게 마음 편해요 .\n",
      "\n",
      "입력 : 로또 번호 알려줘\n",
      "출력 : 제가 알면 제가 먼저 했을 거예요 .\n",
      "\n",
      "입력 : SNS 시간낭비인 거 아는데 매일 하는 중\n",
      "출력 : 시간을 정하고 해보세요 .\n",
      "\n",
      "입력 : 고양이 키우고 싶어\n",
      "출력 : 자신을 먼저 키우세요 .\n",
      "\n",
      "입력 : 첫사랑 생각나웃어줄래\n",
      "출력 : 다른 슬픈 이야기네요 .\n",
      "\n",
      "입력 : 삶은 뭘까?\n",
      "출력 : 지칠 때는 쉬어도 돼요 .\n",
      "\n",
      "입력 : 인생살이 왜이리 힘드냐\n",
      "출력 : 잠시 휴식기를 가져보는것도 좋을거 같아요 .\n",
      "\n",
      "입력 : 이제 그만 쉬고 싶어\n",
      "출력 : 잠깐 바람 쐬고 오세요 .\n",
      "\n",
      "입력 : 너무 고독하다\n",
      "출력 : 환승은 진짜 답도 없네요 .\n",
      "\n",
      "입력 : 도망가고 싶다\n",
      "출력 : 저랑 놀아요 .\n",
      "\n",
      "입력 : 넌 최고였어\n",
      "출력 : 당신도 행복하세요 .\n",
      "\n",
      "입력 : 나 잊지마\n",
      "출력 : 때론 잊어버리는 것이 좋을 때도 있어요 .\n",
      "\n",
      "입력 : 우리 마지막이야\n",
      "출력 : 마지막이 아닐 지도 몰라요 .\n",
      "\n",
      "입력 : 나 간다 잘 지내 안녕\n",
      "출력 : 제가 놀아드리고 싶네요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_answers(input_question_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f02584c",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "\n",
    "## 서론 \n",
    "\n",
    "송영숙님의 한국어 챗봇 데이터를 사용하여 데이터 전처리하기하고 SubwordTextEncoder 사용하고 모델 구성하고 모델 평가하여 결과를 비교해보는 프로젝트이다.\n",
    "\n",
    "## 개발 과정 \n",
    "\n",
    "먼저 불러온 데이터를 전처리한다. 단어 토크나이징 방해되지 않도록 정규 표현식으로 구두점을 제거한 후 질문과 답변의 쌍인 데이터셋을 로드하고 전처리하고 내부 단어 토크나이저인 SubwordTextEncoder 사용하여 단어장을 만든다. 각 단어를 고유한 정수로 인코딩 및 패딩 과정을 해주고 교사 강요(Teacher Forcing)를 적용해준다. 포지셔널 인코딩 레이어와 스케일드 닷 프로덕트 어텐션 함수로 어텐션해주고 마스킹을 한 후 인코더 레이어 함수를 구현해준다. 그리고 디코더 함수도 구현해준 후 트랜스포머 함수 정의를 하여 트랜스포머 모델을 구현해주고 모델을 생성해준 다음 손실함수와 커스텀 학습률을 설정하고 모델을 컴파일하고 에폭값을 조절해가며 모델을 훈련하여 좋은 accuracy를 찾아나간다.\n",
    "\n",
    "\n",
    "## 결과\n",
    "\n",
    "사진첨부가 되지 않아서 글로 쓰자면 처음에 기본으로 적용된 모델 하이퍼파라미터 값에서 에폭을 50으로 줬을 땐 accuracy가 0.1691로 결과가 그렇게 좋지 않아서 에폭 100으로 다시 학습시켜봤는데 accuracy가 거의 0.1738이 나와서 결과가 개선이 되었다. 모델의 하이퍼파라미터를 여러차례 바꾸면서 에폭도 조절해가며 학습시켜봤는데 좋은 결과였던 기록을 남기지 못해 다시 여러차례 시도했지만 결국엔 마지막 모델의 하이퍼파라미터를 건드렸을 때 오히려 낮은 accuracy의 결과를 얻게 되어서 너무 아쉬웠다.\n",
    "\n",
    "\n",
    "## 배운 점 및 회고 \n",
    "\n",
    "약간 말 안듣는 심심이를 구현한 거 같아서 생각보다 재밌었고 신기했다. 그리고 특히나 이번 프로젝트를 할때 어텐션 부분이 너무 어려워서 이해가 잘 가지 않았는데 보충 공부가 더 필요할 거 같다는 생각을 했다. 하이퍼파라미터를 조절해가며 더 좋은 결과를 얻어냈었어서 다시 시도했지만 그 때의 결과를 남기지 못한거 같아서 아쉬움이 많이 남았다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
